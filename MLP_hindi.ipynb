{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "r1H7Jc1mXMFD"
      ],
      "mount_file_id": "1MwlfS3zzy2TjOpUkXpT2LAZ8rips134W",
      "authorship_tag": "ABX9TyPKGeYvCFET11t7JHNflkXc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshitEbPandey/transferLearningNER/blob/mlp_models/MLP_hindi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6ad531MVZ7U"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext"
      ],
      "metadata": {
        "id": "eZ38FJ0aXoNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import keras\n",
        "import fasttext\n",
        "from tensorflow import random\n",
        "from sklearn.metrics import f1_score, classification_report, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "3R6GoqC6Vey3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing and saving txt (X_train_hindi, Y_train_hindi)"
      ],
      "metadata": {
        "id": "r1H7Jc1mXMFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "hi_ner = load_dataset('ai4bharat/naamapadam', 'hi')"
      ],
      "metadata": {
        "id": "-ncOUhzkXaDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analysing the hindi dataset\n",
        "hindi_ds = hi_ner.map()\n",
        "\n",
        "# shapes of each\n",
        "print(f\"Train: {hindi_ds['train'].shape}\")\n",
        "print(f\"Test: {hindi_ds['test'].shape}\")\n",
        "print(f\"Validation: {hindi_ds['validation'].shape}\")\n",
        "\n",
        "# loading each\n",
        "train_hindi_ds = hindi_ds['train'].to_pandas()\n",
        "test_hindi_ds = hindi_ds['test'].to_pandas()\n",
        "vali_hindi_ds = hindi_ds['validation'].to_pandas()"
      ],
      "metadata": {
        "id": "-_dqtwYoXYoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine each of the words in the individual sentences and create a common df\n",
        "# text output is of the form in X_train.txt: `tokens`\n",
        "# text output is of the form in Y_train.txt: `ner_tags`\n",
        "import pandas as pd\n",
        "with open('/content/drive/MyDrive/Spring 2023/CSE 572 data mining/Project/Final/X_train_hindi_ds.txt', 'w') as f1, open('/content/drive/MyDrive/Spring 2023/CSE 572 data mining/Project/Final/Y_train_hindi_ds.txt', 'w') as f2:\n",
        "  for index, row in train_hindi_ds.iterrows():\n",
        "      f1.write('\\n'.join(row['tokens']) + '\\n')\n",
        "      f2.write('\\n'.join(map(str, row['ner_tags'])) + '\\n')"
      ],
      "metadata": {
        "id": "NUUtR12-XLQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the tokens and tags from txt"
      ],
      "metadata": {
        "id": "uKoRZbLFXb5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_loc = '/content/drive/MyDrive/Spring 2023/CSE 572 data mining/Project/Final'"
      ],
      "metadata": {
        "id": "ImSC3Y8qYJ6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all tokens and tags are presaved...\n",
        "# taking them input and making unique dictionary\n",
        "# Open the input file and read the words\n",
        "with open(file_loc + '/X_train_hindi_ds.txt', 'r') as f:\n",
        "    words = f.read().splitlines()"
      ],
      "metadata": {
        "id": "UEybpIjfWTUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_loc + '/Y_train_hindi_ds.txt', 'r') as f:\n",
        "    tags = f.read().splitlines()"
      ],
      "metadata": {
        "id": "kv4qKpd2XGlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tags) == len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVCWZvkdZWn2",
        "outputId": "2aae6f28-44ff-46d2-87e9-cdb4062e5b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tag_unique = {}\n",
        "for idx, word in enumerate(words):\n",
        "  if word not in word_tag_unique:\n",
        "    word_tag_unique[word] = [tags[idx]]\n",
        "  elif word in word_tag_unique and tags[idx] not in word_tag_unique[word]:\n",
        "    word_tag_unique[word].append(tags[idx])"
      ],
      "metadata": {
        "id": "xp5ofH9cZx5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_loc + '/train_hindi_unique_tokens.txt', 'w') as f:\n",
        "  for word in word_tag_unique.keys():\n",
        "    f.write(word + '\\n')"
      ],
      "metadata": {
        "id": "L5AFHpULZbqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training fasttext on unique words dimensions = 30"
      ],
      "metadata": {
        "id": "s92EboqAcpvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run fasttext model\n",
        "model = fasttext.train_unsupervised(file_loc + '/train_hindi_unique_tokens.txt', model='skipgram', lr=0.05, dim=30, ws=5, epoch=5)"
      ],
      "metadata": {
        "id": "Ve9fG5yTZ67p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save_model(file_loc + '/fasttext_model_train_hindi_unique_tokens.bin')"
      ],
      "metadata": {
        "id": "NueRu868cve9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the model\n",
        "model = fasttext.load_model(file_loc + '/fasttext_model_train_hindi_unique_tokens.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1o3xLMbdhVR",
        "outputId": "0522a357-d631-489e-8fa0-7ce36f7dbc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# every word should have an embedding now\n",
        "# can get using model[word]"
      ],
      "metadata": {
        "id": "7szWgxDOdjHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the training set"
      ],
      "metadata": {
        "id": "MlKad04keKM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# constructing x_train_df\n",
        "\n",
        "word_dict = {i: word for i, word in enumerate(words)}\n",
        "X_train_df = pd.DataFrame.from_dict(word_dict, orient='index', columns=['tokens'])"
      ],
      "metadata": {
        "id": "5dJnrRbfeLKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# constructing y_train_df\n",
        "\n",
        "tag_dict = {i: tag for i, tag in enumerate(tags)}\n",
        "Y_train_df = pd.DataFrame.from_dict(tag_dict, orient='index', columns=['tags'])"
      ],
      "metadata": {
        "id": "VtGeu-IpehHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving word vectors in chunks"
      ],
      "metadata": {
        "id": "Rs4b-_Kze5t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_chunk(start_idx, end_idx, df):\n",
        "    # Get the words for the chunk\n",
        "    words = df.loc[start_idx:end_idx, 'tokens'].tolist()\n",
        "\n",
        "   # Get the vectors for the words using the fastText model\n",
        "    vectors = [model[word] for word in words]\n",
        "\n",
        "    # list of index\n",
        "    idx = range(start_idx, end_idx + 1)\n",
        "\n",
        "    # Create a new dataframe with the vectors\n",
        "    vectors_df = pd.DataFrame({'tokens': words, 'vectors': vectors}, index = idx)\n",
        "\n",
        "    # # Save the chunk to a CSV file\n",
        "    vectors_df.to_csv(file_loc + f'/vectors/hindi_lite_vectors_{start_idx}_{end_idx}.csv', index=False)"
      ],
      "metadata": {
        "id": "EAX0k92je75I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_df.shape"
      ],
      "metadata": {
        "id": "HFY96JKugpEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over the dataframe in chunks\n",
        "chunk_size = 1000000\n",
        "for i in range(0, len(X_train_df), chunk_size):\n",
        "    start_idx = i\n",
        "    end_idx = min(i + chunk_size, len(X_train_df)-1)\n",
        "    process_chunk(start_idx, end_idx, X_train_df)\n",
        "    # Print progress\n",
        "    print(f'Saved vectors for words {start_idx} to {end_idx} to CSV file.')"
      ],
      "metadata": {
        "id": "3M2zh08-gmMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(Y_train_df.tags.values)\n",
        "Y_train_hindi_enc = label_encoder.transform(Y_train_df.tags.values)"
      ],
      "metadata": {
        "id": "vWOx0H7Vgz3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get each class' weights\n",
        "unique, counts = np.unique(Y_train_hindi_enc, return_counts=True)\n",
        "class_weights = dict(zip(unique, np.round(sum(counts) / counts)))\n",
        "\n",
        "# more the number of records - lesser the weights are (for balancing dataset)\n",
        "print(class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02OwoH858hoW",
        "outputId": "c588b96e-4335-44b9-bbb5-d067b4a47377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 1.0, 1: 29.0, 2: 31.0, 3: 32.0, 4: 27.0, 5: 30.0, 6: 106.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train_hindi_enc.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6-cl71b8onu",
        "outputId": "87e19c42-f1bd-451b-c56e-a9cd1370e3ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22029408,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training MLP CLassifier"
      ],
      "metadata": {
        "id": "ogzxidY19o-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(Y_train_hindi_enc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHQc1REC-Vrz",
        "outputId": "d2b50c6b-e6c3-4112-a949-e86f3e0da798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = 30\n",
        "print(num_features)\n",
        "num_classes = 7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vpvn0jf49oTL",
        "outputId": "dfe3e731-3267-42f1-8db2-d4d5fb92a7b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed\n",
        "np.random.seed(42)\n",
        "random.set_seed(42)\n",
        "\n",
        "mlp = Sequential()\n",
        "mlp.add(Dense(units=100, activation='relu', input_dim=num_features))\n",
        "mlp.add(Dense(units=num_classes, activation='softmax'))\n",
        "mlp.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5wPQ9nVC-Gxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load test data"
      ],
      "metadata": {
        "id": "SpcXx5Qj-pfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained fastText model\n",
        "model = fasttext.load_model(file_loc + '/fasttext_model_train_hindi_unique_tokens.bin')\n",
        "\n",
        "# Open the input file and read the words\n",
        "with open(file_loc + '/X_test_hindi_ds.txt', 'r') as f:\n",
        "    words = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Create a dictionary with the words as values and integer indices as keys\n",
        "word_dict = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "# Create a dataframe from the dictionary\n",
        "df_test = pd.DataFrame.from_dict(word_dict, orient='index', columns=['tokens'])\n",
        "\n",
        "# Loop over the dataframe in chunks\n",
        "start_idx = 0\n",
        "end_idx = len(df_test) - 1\n",
        "process_chunk(start_idx, end_idx, df_test)\n",
        "# Print progress\n",
        "print(f'Saved test vectors for words {start_idx} to {end_idx} to CSV file.')\n",
        "\n",
        "# get the labels in another dataframe\n",
        "with open(file_loc +'/Y_test_hindi_ds.txt', 'r') as f:\n",
        "    test_tags = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "tag_dict = {i: tag for i, tag in enumerate(test_tags)}\n",
        "\n",
        "df_test_tags = pd.DataFrame.from_dict(tag_dict, orient='index', columns=['tags'])\n",
        "\n",
        "X_test = pd.read_csv(file_loc + f'/vectors/hindi_lite_vectors_{start_idx}_{end_idx}.csv')\n",
        "X_test.head()"
      ],
      "metadata": {
        "id": "kc_hBZGT-o8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing data for prediction\n",
        "\n",
        "target_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6']\n",
        "\n",
        "y_test_hindi_ds = label_encoder.transform(df_test_tags.tags.values)\n",
        "\n",
        "X_test = np.array([np.fromstring(s[1:-1].replace('\\n', ' '), sep=' ') for s in X_test.vectors])\n",
        "\n",
        "Y_test = y_test_hindi_ds\n",
        "\n",
        "y_true_labels = Y_test"
      ],
      "metadata": {
        "id": "ln3EULWi_yyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training model pipeline (in chunks)\n",
        "\n",
        "# Load data in chunks, and train\n",
        "# training sets\n",
        "# Loop over the dataframe in chunks\n",
        "chunk_size = 1000000\n",
        "for i in range(0, len(X_train_df), chunk_size):\n",
        "    start_idx = i\n",
        "    end_idx = min(i + chunk_size, len(X_train_df)-1)\n",
        "    X_train = pd.read_csv(file_loc + f'/vectors/hindi_lite_vectors_{start_idx}_{end_idx}.csv')\n",
        "    X_train = np.array([np.fromstring(s[1:-1].replace('\\n', ' '), sep=' ') for s in X_train.vectors])\n",
        "    Y_train = Y_train_hindi_enc[start_idx: end_idx + 1]\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Loaded vectors for words {start_idx} to {end_idx} to CSV file.')\n",
        "    \n",
        "    # train mlp on this\n",
        "    mlp.fit(X_train, Y_train, class_weight=class_weights, epochs = 50, batch_size = 200)\n",
        "\n",
        "    # predict on test\n",
        "    Y_test_pred = mlp.predict(X_test)\n",
        "\n",
        "    # get pred labels\n",
        "    y_pred_labels = np.argmax(Y_test_pred, axis=1)\n",
        "\n",
        "    # print classification report\n",
        "    print(classification_report(y_true_labels, y_pred_labels, target_names=target_names))\n",
        "\n",
        "    # save model\n",
        "    mlp.save(file_loc + f'/models/mlp_model_hindi_lite_50_epochs.h5')\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Completed training with vectors for words {start_idx} to {end_idx}')"
      ],
      "metadata": {
        "id": "E-CqNZo_8q57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline report 1 :P Did not save the model"
      ],
      "metadata": {
        "id": "7VuVBSMGCJkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "     Class 0       0.92      0.70      0.80      6996\n",
        "     Class 1       0.18      0.42      0.25       263\n",
        "     Class 2       0.21      0.31      0.25       239\n",
        "     Class 3       0.16      0.40      0.23       257\n",
        "     Class 4       0.16      0.41      0.23       253\n",
        "     Class 5       0.27      0.43      0.33       302\n",
        "     Class 6       0.11      0.32      0.16        95\n",
        "    accuracy                           0.65      8405\n",
        "   macro avg       0.28      0.43      0.32      8405\n",
        "weighted avg       0.80      0.65      0.70      8405\n",
        "```\n",
        "Completed training with vectors for words 7000000 to 8000000 (200 epoochs each\n",
        ")"
      ],
      "metadata": {
        "id": "XrIPXwT279gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train for another 100 epochs, last 29000 tokens \n",
        "chunk_size = 1000000\n",
        "for i in range(22000000, len(X_train_df), chunk_size):\n",
        "    start_idx = i\n",
        "    end_idx = min(i + chunk_size, len(X_train_df)-1)\n",
        "    X_train = pd.read_csv(file_loc + f'/vectors/hindi_lite_vectors_{start_idx}_{end_idx}.csv')\n",
        "    X_train = np.array([np.fromstring(s[1:-1].replace('\\n', ' '), sep=' ') for s in X_train.vectors])\n",
        "    Y_train = Y_train_hindi_enc[start_idx: end_idx + 1]\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Loaded vectors for words {start_idx} to {end_idx} to CSV file.')\n",
        "    \n",
        "    # train mlp on this\n",
        "    mlp.fit(X_train, Y_train, class_weight=class_weights, epochs = 150, batch_size = 200)\n",
        "\n",
        "    # predict on test\n",
        "    Y_test_pred = mlp.predict(X_test)\n",
        "\n",
        "    # get pred labels\n",
        "    y_pred_labels = np.argmax(Y_test_pred, axis=1)\n",
        "\n",
        "    # print classification report\n",
        "    print(classification_report(y_true_labels, y_pred_labels, target_names=target_names))\n",
        "\n",
        "    # save model\n",
        "    mlp.save(file_loc + f'/models/mlp_model_hindi_lite_50_epochs_retrain.h5')\n",
        "\n",
        "    # Print progress\n",
        "    print(f'Completed training with vectors for words {start_idx} to {end_idx}')"
      ],
      "metadata": {
        "id": "urG3GoutADnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final good baseline -- results on 50 epochs -- and 150 epoch final 29000 retrained:\n",
        "```\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "     Class 0       0.92      0.71      0.80      6996\n",
        "     Class 1       0.18      0.38      0.25       263\n",
        "     Class 2       0.19      0.31      0.23       239\n",
        "     Class 3       0.13      0.47      0.20       257\n",
        "     Class 4       0.18      0.36      0.24       253\n",
        "     Class 5       0.26      0.41      0.32       302\n",
        "     Class 6       0.15      0.29      0.20        95\n",
        "\n",
        "    accuracy                           0.65      8405\n",
        "   macro avg       0.29      0.42      0.32      8405\n",
        "weighted avg       0.80      0.65      0.71      8405\n",
        "```"
      ],
      "metadata": {
        "id": "SmjRA3MdSah2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test eval on only labels from 1 - 6:\n",
        "# loading the mlp  model\n",
        "mlp=keras.models.load_model('/content/drive/MyDrive/Spring 2023/CSE 572 data mining/Project/Final/models (1)/mlp_model_hindi_lite_50_epochs_retrain.h5')"
      ],
      "metadata": {
        "id": "equKfFQjRMFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_results_hmm(y_test, y_pred, clf_report=True, conf_matrix=False, exclude_0=True):\n",
        "    \"\"\"\n",
        "        Report evaluation metrics for the NER model (HMM)\n",
        "    \"\"\"\n",
        "    labels = np.arange(1,7) if exclude_0 else np.arange(0,7)\n",
        "    print(\"Weighted F1 of HMM = {:.4f}\".format(f1_score(y_test, y_pred, average='weighted', labels=labels)))\n",
        "    if clf_report:\n",
        "        print(classification_report(y_test, y_pred, labels=labels, digits=3, zero_division=0))\n",
        "    if conf_matrix:\n",
        "        ConfusionMatrixDisplay.from_predictions(y_test, y_pred, labels=labels)\n",
        "        plt.show()\n",
        "    print(\"================================================================\\n\")"
      ],
      "metadata": {
        "id": "A4NS2JAbmKcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the input file and read the words\n",
        "with open(file_loc + '/X_test_hindi_ds.txt', 'r') as f:\n",
        "    words = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# get the labels in another dataframe\n",
        "with open(file_loc +'/Y_test_hindi_ds.txt', 'r') as f:\n",
        "    test_tags = [line.strip() for line in f if line.strip()]"
      ],
      "metadata": {
        "id": "cJInydSGnq3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained fastText model\n",
        "model = fasttext.load_model(file_loc + '/fasttext_model_train_hindi_unique_tokens.bin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIqTidyFn6tj",
        "outputId": "236edad2-49cd-4302-f786-82a43d3f1c5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = pd.read_csv(file_loc + f'/vectors/hindi_lite_vectors_{start_idx}_{end_idx}.csv')\n",
        "X_test.head()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(test_tags)\n",
        "test_tags = label_encoder.transform(test_tags)\n",
        "\n",
        "# preparing data for prediction\n",
        "\n",
        "target_names = ['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4', 'Class 5', 'Class 6']\n",
        "\n",
        "y_test_hindi_ds = test_tags\n",
        "\n",
        "X_test = np.array([np.fromstring(s[1:-1].replace('\\n', ' '), sep=' ') for s in X_test.vectors])\n",
        "\n",
        "Y_test = y_test_hindi_ds\n",
        "\n",
        "y_true_labels = Y_test"
      ],
      "metadata": {
        "id": "wA7X1uW0n_Uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # predict on test\n",
        "Y_test_pred = mlp.predict(X_test)\n",
        "\n",
        "# get pred labels\n",
        "y_pred_labels = np.argmax(Y_test_pred, axis=1)\n",
        "\n",
        "# print classification report\n",
        "print(classification_report(y_true_labels, y_pred_labels, target_names=target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF-KCwYMpdZL",
        "outputId": "8e5f7782-29c8-4f51-f6ac-c09b6df06318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "263/263 [==============================] - 1s 1ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class 0       0.92      0.71      0.80      6996\n",
            "     Class 1       0.18      0.38      0.25       263\n",
            "     Class 2       0.19      0.31      0.23       239\n",
            "     Class 3       0.13      0.47      0.20       257\n",
            "     Class 4       0.18      0.36      0.24       253\n",
            "     Class 5       0.26      0.41      0.32       302\n",
            "     Class 6       0.15      0.29      0.20        95\n",
            "\n",
            "    accuracy                           0.65      8405\n",
            "   macro avg       0.29      0.42      0.32      8405\n",
            "weighted avg       0.80      0.65      0.71      8405\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results_hmm(y_true_labels, y_pred_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL0y_NW1p2UD",
        "outputId": "4b887263-a312-4aeb-e471-0113e7e09cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weighted F1 of HMM = 0.2480\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1      0.183     0.376     0.246       263\n",
            "           2      0.187     0.314     0.234       239\n",
            "           3      0.129     0.471     0.203       257\n",
            "           4      0.182     0.356     0.241       253\n",
            "           5      0.262     0.414     0.321       302\n",
            "           6      0.150     0.295     0.199        95\n",
            "\n",
            "   micro avg      0.177     0.382     0.242      1409\n",
            "   macro avg      0.182     0.371     0.241      1409\n",
            "weighted avg      0.188     0.382     0.248      1409\n",
            "\n",
            "================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "agpmU6CvrBHD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}